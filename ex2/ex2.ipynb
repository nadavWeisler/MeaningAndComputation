{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "done1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "print(\"done1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "done2\n"
     ]
    }
   ],
   "source": [
    "# Exports data from corpus\n",
    "sentences, cur_sent = list(), list()\n",
    "with open('./wackypedia_en1.words10.20Mwords') as f:\n",
    " for line in f:\n",
    "    line = line.strip()\n",
    "    if line == '</s>':\n",
    "        sentences.append(cur_sent)\n",
    "        cur_sent = list()\n",
    "    elif line != '<s>' and not line.startswith('<text') and not line.startswith('</text'):\n",
    "        cur_sent.append(line.split('\\t')[0])\n",
    "print(\"done2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "done3\n"
     ]
    }
   ],
   "source": [
    "# Builds the models\n",
    "model_big_ten = gensim.models.Word2Vec(sentences, min_count=5, window=1, vector_size=10)\n",
    "model_small_ten = gensim.models.Word2Vec(sentences, min_count=5, window=10, vector_size=10)\n",
    "\n",
    "model_big_five = gensim.models.Word2Vec(sentences, min_count=5, window=1, vector_size=500)\n",
    "model_small_five = gensim.models.Word2Vec(sentences, min_count=5, window=10, vector_size=500)\n",
    "\n",
    "models = [model_big_ten, model_big_five, model_small_ten, model_small_five]\n",
    "print(\"done3\")"
   ]
  },
  {
   "source": [
    "\n",
    "# Makes the similarity lists\n",
    "words = pd.read_csv('./SimLex-999/SimLex-999.txt', delimiter='\\t')\n",
    "pairs = list(zip(words['word1'],words['word2']))\n",
    "print(\"done4\")"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "done4\n"
     ]
    }
   ]
  },
  {
   "source": [
    "results = [[], [], [], []]\n",
    "for j in range(len(models)):\n",
    "    for i in range(len(pairs)):\n",
    "        try:\n",
    "            similarity = models[j].wv.similarity(pairs[i][0], pairs[i][1])\n",
    "        except KeyError:\n",
    "            similarity = 0\n",
    "        finally:\n",
    "            results[j].append(similarity)\n",
    "print(\"done5\")"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "done5\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.1254553902335779, 0.21405584042237163, 0.19515497539051918, 0.2314181840859071]\n"
     ]
    }
   ],
   "source": [
    "cor_results = []\n",
    "golden_sim = words['SimLex999']\n",
    "for result in results:\n",
    "    sim_res, pval = sp.stats.spearmanr(result, golden_sim)\n",
    "    cor_results.append(sim_res)\n",
    "print(cor_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.12137775701718927, 0.20485917710660673, 0.23085731623835506, 0.2457532439538194]\n"
     ]
    }
   ],
   "source": [
    "pos = words['POS']\n",
    "n_results = [[], [], [], []]\n",
    "\n",
    "for j in range(len(models)):\n",
    "    for i in range(len(pairs)):\n",
    "        if pos[i] == 'N':\n",
    "            try:\n",
    "                similarity = models[j].wv.similarity(pairs[i][0], pairs[i][1])\n",
    "            except KeyError:\n",
    "                similarity = 0\n",
    "            finally:\n",
    "                n_results[j].append(similarity)\n",
    "\n",
    "n_golden = []\n",
    "for i in range(len(pairs)):\n",
    "    if pos[i] == 'N':\n",
    "        n_golden.append(words['SimLex999'][i])\n",
    "\n",
    "n_cor_results = []\n",
    "\n",
    "for result in n_results:\n",
    "    sim_res, pval = sp.stats.spearmanr(result, n_golden)\n",
    "    n_cor_results.append(sim_res)\n",
    "print(n_cor_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.210399531808501, 0.304807496739195, 0.22041688558879352, 0.2646810401816938]\n"
     ]
    }
   ],
   "source": [
    "pos = words['POS']\n",
    "n_results = [[], [], [], []]\n",
    "\n",
    "for j in range(len(models)):\n",
    "    for i in range(len(pairs)):\n",
    "        if pos[i] == 'A':\n",
    "            try:\n",
    "                similarity = models[j].wv.similarity(pairs[i][0], pairs[i][1])\n",
    "            except KeyError:\n",
    "                similarity = 0\n",
    "            finally:\n",
    "                n_results[j].append(similarity)\n",
    "\n",
    "n_golden = []\n",
    "for i in range(len(pairs)):\n",
    "    if pos[i] == 'A':\n",
    "        n_golden.append(words['SimLex999'][i])\n",
    "\n",
    "n_cor_results = []\n",
    "\n",
    "for result in n_results:\n",
    "    sim_res, pval = sp.stats.spearmanr(result, n_golden)\n",
    "    n_cor_results.append(sim_res)\n",
    "print(n_cor_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.08430076723929227, 0.22766844901409633, 0.09382735281082513, 0.19888961622083406]\n"
     ]
    }
   ],
   "source": [
    "pos = words['POS']\n",
    "n_results = [[], [], [], []]\n",
    "\n",
    "for j in range(len(models)):\n",
    "    for i in range(len(pairs)):\n",
    "        if pos[i] == 'V':\n",
    "            try:\n",
    "                similarity = models[j].wv.similarity(pairs[i][0], pairs[i][1])\n",
    "            except KeyError:\n",
    "                similarity = 0\n",
    "            finally:\n",
    "                n_results[j].append(similarity)\n",
    "\n",
    "n_golden = []\n",
    "for i in range(len(pairs)):\n",
    "    if pos[i] == 'V':\n",
    "        n_golden.append(words['SimLex999'][i])\n",
    "\n",
    "n_cor_results = []\n",
    "\n",
    "for result in n_results:\n",
    "    sim_res, pval = sp.stats.spearmanr(result, n_golden)\n",
    "    n_cor_results.append(sim_res)\n",
    "print(n_cor_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python395jvsc74a57bd0606544874517a93eeb5c4fd8b567e16aa4bb33c2bfdc9d3fad8c7c7e88f54736",
   "display_name": "Python 3.9.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}