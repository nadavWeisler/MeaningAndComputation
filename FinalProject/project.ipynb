{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\r\n",
    "from sklearn.pipeline import Pipeline\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "import pandas as pd\r\n",
    "import gensim\r\n",
    "import scipy as sp\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from nltk.tokenize import word_tokenize"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "D:\\Users\\yelfs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Base Run"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "all_data = pd.read_csv(\"./data/Emotion_final.csv\")\r\n",
    "andbrain_data =pd.read_csv(\"./data/Andbrain_DataSet.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "shuffled_data = all_data.sample(frac=1).reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "y = shuffled_data[\"label\"]\r\n",
    "x = shuffled_data\r\n",
    "\r\n",
    "x_train,x_test,y_train,y_test = train_test_split(x, y, test_size=0.2, random_state=42)\r\n",
    "\r\n",
    "y_train = y_train.to_frame()\r\n",
    "y_test = y_test.to_frame()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "algorithms = [RandomForestClassifier(), LogisticRegression()]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "source": [
    "count_vectorizer=CountVectorizer(analyzer='word', ngram_range=(1, 1))\r\n",
    "x = count_vectorizer.fit_transform(x_train[\"text\"])\r\n",
    "print(x.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(15854, 16307)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def calculate_accuracy(predicted_data):\r\n",
    "    count_equals = 0\r\n",
    "    for idx, (_, row) in enumerate(x_test.iterrows()):\r\n",
    "        if row[\"label\"] == predicted_data[idx]:\r\n",
    "            count_equals += 1\r\n",
    "    print(f\"accuracy={count_equals/len(predicted_data)}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "source": [
    "for algorithm_under_test in algorithms:\r\n",
    "    pipe = Pipeline([('vectorizer', count_vectorizer),\r\n",
    "                    ('algo', algorithm_under_test)])\r\n",
    "    pipe.fit(x_train[\"text\"], y_train[\"label\"])\r\n",
    "    predicted = pipe.predict(x_test[\"text\"])\r\n",
    "    calculate_accuracy(predicted)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "accuracy=0.8915237134207871\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "D:\\Users\\yelfs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "accuracy=0.9132189707366297\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Processing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "train_data_list, cur_sent = list(), list()\r\n",
    "with open('./data/wackypedia_en1.words10.20Mwords') as f:\r\n",
    " for line in f:\r\n",
    "    line = line.strip()\r\n",
    "    if line == '</s>':\r\n",
    "        train_data_list.append(cur_sent)\r\n",
    "        cur_sent = list()\r\n",
    "    elif line != '<s>' and not line.startswith('<text') and not line.startswith('</text'):\r\n",
    "        cur_sent.append(line.split('\\t')[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Builds the models\r\n",
    "\r\n",
    "# model_big_ten = gensim.models.Word2Vec(\r\n",
    "#     train_data_list, min_count=5, window=1, vector_size=10)\r\n",
    "# model_small_ten = gensim.models.Word2Vec(\r\n",
    "#     train_data_list, min_count=5, window=10, vector_size=10)\r\n",
    "\r\n",
    "# model_big_five = gensim.models.Word2Vec(\r\n",
    "#     train_data_list, min_count=5, window=1, vector_size=500)\r\n",
    "model_small_five = gensim.models.Word2Vec(\r\n",
    "    train_data_list, min_count=5, window=10, vector_size=500)\r\n",
    "\r\n",
    "models = [model_small_five]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def get_without_stopwords(sentence):\r\n",
    "\tword_tokens = word_tokenize(sentence)\r\n",
    " \r\n",
    "\tfiltered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\r\n",
    "\t\r\n",
    "\tfiltered_sentence = []\r\n",
    "\t\r\n",
    "\tfor w in word_tokens:\r\n",
    "\t\tif w not in stop_words:\r\n",
    "\t\t\tfiltered_sentence.append(w)\r\n",
    "\treturn filtered_sentence"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "emotions = dict()\r\n",
    "emotions[\"happy\"] = 0\r\n",
    "emotions[\"sadness\"] = 1\r\n",
    "emotions[\"anger\"] = 2\r\n",
    "emotions[\"fear\"] = 3\r\n",
    "emotions[\"surprise\"] = 4"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def get_word_score(word):\r\n",
    "\tscores = [0, 0, 0, 0, 0]\r\n",
    "\tsimilar_words = models[0].wv.most_similar(word)[:3]\r\n",
    "\tsimilar_words.append((word, 1.0))\r\n",
    "\tfor word_candidate, correlation in similar_words:\r\n",
    "\t\ttry:\r\n",
    "\t\t\tword_score_row = andbrain_data.loc[andbrain_data['word'] == word_candidate + \" \"]\r\n",
    "\t\t\tif not word_score_row.empty:\r\n",
    "\t\t\t\tcolumns = list(word_score_row)\r\n",
    "\t\t\t\tfor column in columns:\r\n",
    "\t\t\t\t\tif column != 'word':\r\n",
    "\t\t\t\t\t\tscores[emotions[column]] += word_score_row[column].values[0]\r\n",
    "\t\texcept KeyError as e:\r\n",
    "\t\t\tcontinue\r\n",
    "\treturn scores\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def is_all_emotions_zero(word_score):\r\n",
    "\tresult = True\r\n",
    "\tfor score in word_score:\r\n",
    "\t\tif score != 0:\r\n",
    "\t\t\tresult = False\r\n",
    "\t\t\tbreak\r\n",
    "\treturn result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def get_sentence_score(word_tokens):\r\n",
    "\tsentence_score = [0, 0, 0, 0, 0]\r\n",
    "\tfor word in word_tokens:\r\n",
    "\t\ttry:\r\n",
    "\t\t\tword_score = get_word_score(word)\r\n",
    "\t\t\tif not is_all_emotions_zero(word_score):\r\n",
    "\t\t\t\tsentence_score[word_score.index(max(word_score))] += 1\r\n",
    "\t\t\t# word_score_row = andbrain_data.loc[andbrain_data['word'] == word + \" \"]\r\n",
    "\t\t\t# if not word_score_row.empty:\r\n",
    "\t\t\t# \tprint(models[0].wv.most_similar(word)[:3])\r\n",
    "\t\texcept KeyError:\r\n",
    "\t\t\tcontinue\r\n",
    "\treturn sentence_score\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def get_vector_with_score(vector, raw_sentence):\r\n",
    "\tnon_stopwords = get_without_stopwords(raw_sentence)\r\n",
    "\tsentence_score = np.array(get_sentence_score(non_stopwords))\r\n",
    "\treturn np.concatenate((vector, sentence_score))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "source": [
    "def get_processed_data(data_to_process, title):\r\n",
    "\tcount_vectorizer=CountVectorizer(analyzer='word', ngram_range=(1, 1))\r\n",
    "\tvectors = count_vectorizer.fit_transform(data_to_process[\"text\"])\r\n",
    "\t# new_count_vector = pd.DataFrame(columns=[\"vector\"])\r\n",
    "\tnew_count_vector = np.zeros(shape=(data_to_process.shape[0], vectors.shape[1] + 5))\r\n",
    "\tprint(\"WORKING ON \" + title)\r\n",
    "\tfor idx, vector in enumerate(vectors):\r\n",
    "\t\tprint(str(idx) + \" OUT OF \" + str(vectors.shape[0]))\r\n",
    "\t\traw_sentence = data_to_process.iloc[[idx]]\t\r\n",
    "\t\tnew_vector_data = get_vector_with_score(vector.toarray()[0], raw_sentence.values[0][0])\r\n",
    "\t\tnew_count_vector[idx] = new_vector_data\r\n",
    "\t\tprint(new_count_vector[idx])\r\n",
    "\t\t# new_count_vector = np.append(new_count_vector, new_vector_data, axis=1)\r\n",
    "\t\t# new_count_vector.append(new_vector_data)\r\n",
    "\tprint(\"DONE WITH \" + title, end=\"\\n\")\r\n",
    "\treturn new_count_vector"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "source": [
    "processed_train = get_processed_data(x_train, \"TRAIN\")\r\n",
    "processed_test = get_processed_data(x_test, \"TEST\")"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.93 GiB for an array with shape (15854, 16312) and data type float64",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-128-045abbbc9705>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprocessed_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_processed_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"TRAIN\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprocessed_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_processed_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"TEST\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-127-6b30f87d0245>\u001b[0m in \u001b[0;36mget_processed_data\u001b[1;34m(data_to_process, title)\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_to_process\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;31m# new_count_vector = pd.DataFrame(columns=[\"vector\"])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mnew_count_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_to_process\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"WORKING ON \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.93 GiB for an array with shape (15854, 16312) and data type float64"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(processed_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "for algorithm_under_test in algorithms:\r\n",
    "    pipe = Pipeline([('algo', algorithm_under_test)])\r\n",
    "    pipe.fit(processed_train_shaped, y_train[\"label\"])\r\n",
    "    predicted = pipe.predict(processed_test)\r\n",
    "    calculate_accuracy(predicted)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-198-4eda18a3e462>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0malgorithm_under_test\u001b[0m \u001b[1;32min\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mpipe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'algo'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgorithm_under_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_train_shaped\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"label\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mcalculate_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\yelfs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'passthrough'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\yelfs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    303\u001b[0m             )\n\u001b[0;32m    304\u001b[0m         X, y = self._validate_data(X, y, multi_output=True,\n\u001b[1;32m--> 305\u001b[1;33m                                    accept_sparse=\"csc\", dtype=DTYPE)\n\u001b[0m\u001b[0;32m    306\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m             \u001b[0msample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\yelfs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\yelfs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\yelfs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    876\u001b[0m                     \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    877\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 878\u001b[1;33m                     estimator=estimator)\n\u001b[0m\u001b[0;32m    879\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    880\u001b[0m         y = check_array(y, accept_sparse='csr', force_all_finite=True,\n",
      "\u001b[1;32mD:\\Users\\yelfs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\yelfs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    727\u001b[0m                              \u001b[1;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m                              % (n_samples, array.shape, ensure_min_samples,\n\u001b[1;32m--> 729\u001b[1;33m                                 context))\n\u001b[0m\u001b[0;32m    730\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required."
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.8 64-bit"
  },
  "interpreter": {
   "hash": "b1411b1b95c1bf0479e3d7cdc7e102371516c7735698e266ce38042d8c148f2e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}